{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844a7812",
   "metadata": {},
   "source": [
    "# Deploy a pretrained PyTorch BERT model from Hugging Face Hub on Amazon SageMaker\n",
    "\n",
    "Sentiment analysis is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
    "\n",
    "BERT was trained on BookCorpus and English Wikipedia data, which contain 800 million words and 2,500 million words, respectively. Training BERT from scratch would be prohibitively expensive. By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a relatively small amount of training data to achieve state-of-the-art results for common NLP tasks, such as text classification and question answering.\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK provides open source APIs and containers that make it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks.\n",
    "\n",
    "Our customers often ask for quick fine-tuning and easy deployment of their NLP models.\n",
    "\n",
    "In this notebook, you will deploy a pretrained PyTorch BERT model from Hugging Face Hub on Amazon SageMaker\n",
    "\n",
    "You'll execute the following steps:\n",
    " - Initiate a [Huggingface pipeline](https://huggingface.co/transformers/main_classes/pipelines.html) and save the model and config on the local file system.\n",
    " - Tar GZIP the model and config files and upload `model.tar.gz` to a S3 bucket.\n",
    " - Deploy the model to a SageMaker Endpoint and make few inference requests.\n",
    " - Optional cleanup.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "HeBERT: Pre-trained BERT for Polarity Analysis and Emotion Recognition\n",
    "\n",
    "Chriqui, A., & Yahav, I. (2021). HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis and Emotion Recognition. arXiv preprint arXiv:2102.01909.\n",
    "```\n",
    "@article{chriqui2021hebert,\n",
    "  title={HeBERT \\& HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis and Emotion Recognition},\n",
    "  author={Chriqui, Avihay and Yahav, Inbal},\n",
    "  journal={arXiv preprint arXiv:2102.01909},\n",
    "  year={2021}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4a74d",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af937170",
   "metadata": {},
   "source": [
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for the model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = (\n",
    "    \"sagemaker/hebert-sentiment-analysis-pytorch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13475b75",
   "metadata": {},
   "source": [
    "## Initiate a Huggingface pipeline\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the [task summary](https://huggingface.co/transformers/task_summary.html) for examples of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec440263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"avichr/heBERT_sentiment_analysis\",\n",
    "    tokenizer=\"avichr/heBERT_sentiment_analysis\",\n",
    "    return_all_scores = True\n",
    ")\n",
    "\n",
    "sentiment_analysis.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e49f7",
   "metadata": {},
   "source": [
    "## Upload the pre-trained model to S3\n",
    "\n",
    "No you can see that there is a pretrained BERT model under `model` directory by listing the files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rtlh ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6d07d",
   "metadata": {},
   "source": [
    "Now you'll create a `model.tar.gz` file to be used by SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd model && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad573925",
   "metadata": {},
   "source": [
    "Upload the `model.tar.gz` to the bucket in S3 you previously setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", 'rb')\n",
    "key= os.path.join(prefix, 'model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)\n",
    "print(os.path.join(bucket,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f78432",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_data = 's3://{}/{}'.format(bucket,key)\n",
    "pretrained_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f31ce5",
   "metadata": {},
   "source": [
    "## Host\n",
    "\n",
    "### Create endpoint\n",
    "\n",
    "To deploy a pretrained `PyTorch` model, you'll need to use the `PyTorch` estimator object to create a `PyTorchModel` object and set a different `entry_point`.\n",
    "\n",
    "You'll use the `PyTorchModel` object to deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of `model_fn` is required for inference script. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `model_fn` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "Here's an example of the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb491e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5160bf",
   "metadata": {},
   "source": [
    "## Create a model object\n",
    "\n",
    "You define the model object by using the SageMaker Python SDK's `PyTorchModel` and pass in the model from the `estimator` and the `entry_point`. The endpoint's entry point for inference is defined by `model_fn` as seen in the following code block that prints out `inference.py`. The function loads the model and sets it to use a GPU, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel \n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=pretrained_model_data,\n",
    "                             role=role,\n",
    "                             framework_version=\"1.7.1\",\n",
    "                             source_dir=\"code\",\n",
    "                             py_version=\"py3\",\n",
    "                             entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a88d4",
   "metadata": {},
   "source": [
    "## Deploy in SageMaker endpoint\n",
    "\n",
    "After you test the endpoint locally, you will deploy it on SageMaker by simply changing the instance type from local to the valid EC2 instance type. For example, `ml.m5.large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91491c0",
   "metadata": {},
   "source": [
    "Since in the `input_fn` we declared that the incoming requests are json-encoded, we need use a json serializer,\n",
    "to encode the incoming data into a json string. Also, we declared the return content type to be json string, we\n",
    "need to use a json deserializer to parse the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf672e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862953d",
   "metadata": {},
   "source": [
    "## Test the model in SageMaker Endpoint \n",
    "\n",
    "You can test the depolyed model using few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be274a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\"אני מתלבט מה לאכול לארוחת צהריים\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f980bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\"אני לא אוהב שיש באגים \")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e15ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\"אני אוהב לעבוד באמזון\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3bf48",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed. Here we'll also delete the endpoint configuration - to keep things tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27720f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89720b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
