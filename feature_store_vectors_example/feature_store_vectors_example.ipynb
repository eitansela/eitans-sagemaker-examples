{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker FeatureStore - Example with Vectors\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook.\n",
    "\n",
    "In this notebook, you will learn how to create a feature group for a dummy created large dataset in the SageMaker Feature Store with vectors Base64 encoded. You will then learn how to ingest the feature columns into the created feature group (both the Online and the Offline store) using SageMaker Python SDK. You will also see how to get an ingested feature record from the Online store and perform an SQL query to fetch from Offline feature store using Athena API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker FeatureStore\n",
    "\n",
    "Let's start by setting up the SageMaker Python SDK and boto client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install sagemaker pandas numpy --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 Bucket Setup For The OfflineStore\n",
    "\n",
    "SageMaker FeatureStore writes the data in the OfflineStore of a FeatureGroup to a S3 bucket owned by you. To be able to write to your S3 bucket, SageMaker FeatureStore assumes an IAM role which has access to it. The role is also owned by you.\n",
    "Note that the same bucket can be re-used across FeatureGroups. Data in the bucket is partitioned by FeatureGroup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default s3 bucket name and it will be referenced throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = 'sagemaker-featurestore-large-vecors-demo'\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the IAM role. This role gives SageMaker FeatureStore access to your S3 bucket. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> In this example we use the default SageMaker role, assuming it has both <b>AmazonSageMakerFullAccess</b> and <b>AmazonSageMakerFeatureStoreAccess</b> managed policies. If not, please make sure to attach them to the role before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
    "role = get_execution_role()\n",
    "print (role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset\n",
    "\n",
    "We create a large dataset with `list_size` size `dataframe`, with vectors `vector_size` size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = 10000\n",
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a random embeddings with 128 chars length and then create a dummy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "embeddings = []\n",
    "for _ in range(list_size):\n",
    "    embeddings.append(np.random.rand(vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_id_list = [i+1 for i in range(list_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "transaction_amt_list = [random.randrange(1000, 10000) for i in range(list_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_list = [random.randrange(1, 4) for i in range(list_size)]\n",
    "\n",
    "for i in range(list_size):\n",
    "    if cards_list[i]==1:\n",
    "        cards_list[i]=\"mastercard\"\n",
    "    if cards_list[i]==2:\n",
    "        cards_list[i]=\"visa\"\n",
    "    if cards_list[i]==3:\n",
    "        cards_list[i]=\"diners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'TransactionID': transaction_id_list, \n",
    "    'TransactionAmt': transaction_amt_list, \n",
    "    'card': cards_list,\n",
    "    'embeddings': embeddings\n",
    "}\n",
    "my_sample_data = pd.DataFrame(data=d)\n",
    "my_sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data into FeatureStore\n",
    "\n",
    "In this step we will create the FeatureGroups representing the transaction and identity tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "my_sample_feature_group_name = 'my-sample-large-vectors-b64-feature-group-'+str(list_size) + '-'+str(vector_size) + '-'+strftime('%d-%H-%M-%S', gmtime())\n",
    "my_sample_feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "my_sample_feature_group = FeatureGroup(name=my_sample_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Vector Features to Base64\n",
    "\n",
    "In order to save space and improve latency, we encode features which are vectors to Base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_store_utils import encode_vector_features\n",
    "\n",
    "encode_vector_features(my_sample_data, ['embeddings'])\n",
    "my_sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Object to String\n",
    "\n",
    "We will now cast all object dtypes to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_store_utils import cast_object_to_string\n",
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "cast_object_to_string(my_sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `EventTime` needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"TransactionID\"\n",
    "event_time_feature_name = \"EventTime\"\n",
    "\n",
    "# append EventTime feature\n",
    "my_sample_data[event_time_feature_name] = pd.Series([current_time_sec]*len(my_sample_data), dtype=\"float64\")\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "my_sample_feature_group.load_feature_definitions(data_frame=my_sample_data); # output is suppressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Feature Group in SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "my_sample_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=my_sample_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the FeatureGroup has been created by using the DescribeFeatureGroup and ListFeatureGroups APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_describe_response = my_sample_feature_group.describe()\n",
    "feature_group_describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into Feature Group\n",
    "\n",
    "After the Feature Group has been created, we can put data into the Feature Group by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "my_sample_feature_group.ingest(\n",
    "    data_frame=my_sample_data, max_workers=5, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that data has been ingested, we can quickly retrieve a record from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = str(1)\n",
    "\n",
    "featurestore_runtime.get_record(FeatureGroupName=my_sample_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve a record from the online store into a Pandas DataFrame \n",
    "\n",
    "We'll instantiate `FeatureStoreUtils` class and use the `get_record_to_df` function in the `feature_store_utils` helper file to retrieve a record from the online store into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_store_utils import FeatureStoreUtils\n",
    "my_sample_feature_store_utils = FeatureStoreUtils(FeatureGroupName=my_sample_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = 3\n",
    "\n",
    "df = my_sample_feature_store_utils.get_record_to_df(RecordIdentifierValueAsString=record_identifier_value)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's decode the features which are vectors back from Base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_store_utils import decode_vector_features\n",
    "\n",
    "decode_vector_features(df, ['embeddings'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch fetch multiple records from the Online Feature Store\n",
    "\n",
    "Fetch a list of selected items from the feature group.\n",
    "##### Up to 100 records can be fetched from an online Feature Store in a single batch operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = [\n",
    "    {\n",
    "        'FeatureGroupName': my_sample_feature_group_name,\n",
    "        'RecordIdentifiersValueAsString': [\"1\", \"2\",\"3\"]\n",
    "    }\n",
    "]\n",
    "        \n",
    "batch_get_record_response = featurestore_runtime.batch_get_record(Identifiers=identifiers)\n",
    "records = batch_get_record_response['Records']\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch a list of selected items from a single feature group into a Pandas DataFrame\n",
    "\n",
    "We'll use the `batch_get_records_to_df` function in the `feature_store_utils` helper file to fetch a list of selected items from a single feature group into a Pandas DataFrame, and then use `decode_vector_features` to decode the features which are vectors back from Base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifiers_list = [\"1\", \"2\",\"3\"]\n",
    "\n",
    "batch_df = my_sample_feature_store_utils.batch_get_records_to_df(RecordIdentifiersValueAsString=record_identifiers_list)\n",
    "decode_vector_features(batch_df, ['embeddings'])\n",
    "batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a record in the Feature Store\n",
    "\n",
    "We will update the record with `TransactionID` value of 1 with another `TransactionAmt` and update the `EventTime`.\n",
    "\n",
    "Let's quickly retrieve this record from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = 1\n",
    "my_sample_record = my_sample_feature_store_utils.get_record_to_df(RecordIdentifierValueAsString=record_identifier_value)\n",
    "my_sample_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_time_sec = int(round(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_record[\"EventTime\"] = updated_time_sec\n",
    "my_sample_record[\"EventTime\"] = my_sample_record[\"EventTime\"].astype(\"float64\")\n",
    "my_sample_record[\"TransactionAmt\"] = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ingest the updated record into feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "my_sample_feature_group.ingest(\n",
    "    data_frame=my_sample_record, max_workers=1, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the record has been ingested and the data was updated, we can quickly retrieve this record from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = 1\n",
    "my_sample_feature_store_utils.get_record_to_df(RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a record from the Feature Store\n",
    "\n",
    "We are now going to delete this record from the FeatureGroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_time_sec = str(round(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_record_response = featurestore_runtime.delete_record(\n",
    "    FeatureGroupName=my_sample_feature_group_name,\n",
    "    RecordIdentifierValueAsString=str(record_identifier_value),\n",
    "    EventTime=deleted_time_sec\n",
    ")\n",
    "delete_record_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the record was deleted, we can quickly try to retrieve this record from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = 1\n",
    "my_sample_feature_store_utils.get_record_to_df(RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker Python SDK’s FeatureStore class also provides the functionality to generate Hive DDL commands. Schema of the table is generated based on the feature definitions. Columns are named after feature name and data-type are inferred based on feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_sample_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wait for the data to appear in our offline store before moving forward to creating a dataset. This will take approximately 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_offline_location=feature_group_describe_response['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "print(s3_offline_location)\n",
    "\n",
    "my_sample_feature_group_s3_prefix = '/'.join(s3_offline_location.split(\"/\")[3:])\n",
    "print(my_sample_feature_group_s3_prefix)\n",
    "\n",
    "offline_store_contents = None\n",
    "while (offline_store_contents is None):\n",
    "    objects_in_bucket = s3_client.list_objects(Bucket=default_s3_bucket_name,Prefix=my_sample_feature_group_s3_prefix)\n",
    "    if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "        offline_store_contents = objects_in_bucket['Contents']\n",
    "    else:\n",
    "        print('Waiting for data in offline store...\\n')\n",
    "        sleep(60)\n",
    "    \n",
    "print('Data available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that fetch the data stored in the offline store in S3 for the FeatureGroup. \n",
    "\n",
    "Note the `eventtime` and `is_deleted` columns for the records with `TransactionID` value of 1 that were fetched. A new record is showing up now with a value of `True` in the `is_deleted` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_query = my_sample_feature_group.athena_query()\n",
    "\n",
    "my_sample_table = my_sample_query.table_name\n",
    "\n",
    "query_string = 'SELECT * FROM \"'+my_sample_table+'\" order by transactionid'\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "#dataset = pd.DataFrame()\n",
    "my_sample_query.run(query_string=query_string, output_location='s3://'+default_s3_bucket_name+'/'+prefix+'/query_results/')\n",
    "my_sample_query.wait()\n",
    "dataset = my_sample_query.as_dataframe()\n",
    "\n",
    "decode_vector_features(dataset, ['embeddings'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources\n",
    "\n",
    "Before deleting the Feature Group, let's inspect data size in the Offline Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_s3_uri = feature_group_describe_response['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "print(fs_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {fs_s3_uri}/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls ${fs_s3_uri}/ --recursive | awk '{total += $3} END {print \"Total:\", total/1048576, \"MB\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Offline Feature Group data is around 177 MB. Le't delete it before we delete the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm {fs_s3_uri}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's delete the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_feature_group.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}