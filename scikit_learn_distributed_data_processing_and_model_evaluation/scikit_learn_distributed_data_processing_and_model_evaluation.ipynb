{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Distributed Processing jobs\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "<img src=\"Processing-1.jpg\">\n",
    "\n",
    "This notebook shows how you can run a distributed processing job to run a scikit-learn script that cleans, pre-processes, performs feature engineering, and splits the input data into train and test sets.\n",
    "\n",
    "The dataset used here is the [Census-Income KDD Dataset](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29). You select features from this dataset, clean the data, and turn the data into features that the training algorithm can use to train a binary classification model, and split the data into train and test sets. The task is to predict whether rows representing census responders have an income greater than `$50,000`, or less than `$50,000`. The dataset is heavily class imbalanced, with most records being labeled as earning less than `$50,000`. After training a logistic regression model, you evaluate the model against a hold-out test dataset, and save the classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the scikit-learn preprocessing script as a processing job, create a `SKLearnProcessor`, which lets you run scripts inside of processing jobs using the scikit-learn image provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the dataset into three different prefixes in the same bucket. this way, every instance of the SageMaker Processing jobs will process one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "source= { 'Bucket' : 'sagemaker-sample-data-{}'.format(region), 'Key': 'processing/census/census-income.csv'}\n",
    "dest = s3.Bucket(bucket)\n",
    "dest.copy(source, 'sagemaker/dist-processing/census/001/census-income.csv')\n",
    "dest.copy(source, 'sagemaker/dist-processing/census/002/census-income.csv')\n",
    "dest.copy(source, 'sagemaker/dist-processing/census/003/census-income.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing the script you use for data cleaning, pre-processing, and feature engineering, inspect the first 20 rows of the dataset in `001` prefix. The target is predicting the `income` category. The features from the dataset you select are `age`, `education`, `major industry code`, `class of worker`, `num persons worked for employer`, `capital gains`, `capital losses`, and `dividends from stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prefix = \"sagemaker/processing/census/\"\n",
    "\n",
    "input_folder = 's3://{}/sagemaker/dist-processing/census/'.format(bucket)\n",
    "input_file = '{}001/census-income.csv'.format(input_folder)\n",
    "\n",
    "df = pd.read_csv(input_file, nrows=10)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun this cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, you\n",
    "\n",
    "* Remove duplicates and rows with conflicting data\n",
    "* transform the target `income` column into a column containing two labels.\n",
    "* transform the `age` and `num persons worked for employer` numerical columns into categorical features by binning them\n",
    "* scale the continuous `capital gains`, `capital losses`, and `dividends from stocks` so they're suitable for training\n",
    "* encode the `education`, `major industry code`, `class of worker` so they're suitable for training\n",
    "* split the data into training and test datasets, and saves the training features and labels and test features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "INPUT_PATH = '/opt/ml/processing/input/'\n",
    "\n",
    "columns = ['age', 'education', 'major industry code', 'class of worker', 'num persons worked for employer',\n",
    "           'capital gains', 'capital losses', 'dividends from stocks', 'income']\n",
    "class_labels = [' - 50000.', ' 50000+.']\n",
    "\n",
    "s3_client = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def upload_objects(bucket, prefix, local_path):\n",
    "    try:\n",
    "        bucket_name = bucket  # s3 bucket name\n",
    "        root_path = local_path  # local folder for upload\n",
    "\n",
    "        print('bucket_name: {}'.format(bucket_name))\n",
    "        print('root_path: {}'.format(root_path))\n",
    "        \n",
    "        s3_bucket = s3_client.Bucket(bucket_name)\n",
    "\n",
    "        for path, subdirs, files in os.walk(root_path):\n",
    "            for file in files:\n",
    "                print('Uploading file: {}'.format(os.path.join(path, file)))               \n",
    "                s3_bucket.upload_file(\n",
    "                    os.path.join(path, file), \"sagemaker/dist-processing/census/output/{}/{}\".format(prefix, file)\n",
    "                )\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        \n",
    "        \n",
    "def print_shape(df):\n",
    "    negative_examples, positive_examples = np.bincount(df['income'])\n",
    "    print('Data shape: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "    print('Received arguments: {}'.format(args))\n",
    "\n",
    "    # Creating the necessary paths to save the output files\n",
    "    if not os.path.exists(\"/opt/ml/processing/train\"):\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "\n",
    "    if not os.path.exists(\"/opt/ml/processing/test\"):\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "    \n",
    "    prefix = [folder for folder in os.listdir(INPUT_PATH) if folder.startswith('0')][0]\n",
    "    \n",
    "    print('Received prefix: {}'.format(prefix))\n",
    "    print('Files in Input Directory: {}'.format(os.listdir(INPUT_PATH + prefix)))\n",
    "\n",
    "    input_data_path = os.path.join(INPUT_PATH + prefix, 'census-income.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df, columns=columns)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.replace(class_labels, [0, 1], inplace=True)\n",
    "    \n",
    "    negative_examples, positive_examples = np.bincount(df['income'])\n",
    "    print('Data after cleaning: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    print('Splitting data into train and test sets with ratio {}'.format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('income', axis=1), df['income'], test_size=split_ratio, random_state=0)\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        (['age', 'num persons worked for employer'], KBinsDiscretizer(encode='onehot-dense', n_bins=10)),\n",
    "        (['capital gains', 'capital losses', 'dividends from stocks'], StandardScaler()),\n",
    "        (['education', 'major industry code', 'class of worker'], OneHotEncoder(sparse=False))\n",
    "    )\n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = preprocess.fit_transform(X_train)\n",
    "    test_features = preprocess.transform(X_test)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n",
    "    \n",
    "    upload_objects(\n",
    "        args['s3_output_bucket'],\n",
    "        prefix,\n",
    "        \"/opt/ml/processing/train/\",\n",
    "    )\n",
    "    upload_objects(\n",
    "        args['s3_output_bucket'],\n",
    "        prefix,\n",
    "        \"/opt/ml/processing/test/\",\n",
    "    )\n",
    "    \n",
    "    print('Processing Complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job. Use the `SKLearnProcessor.run()` method. You give the `run()` method one `ProcessingInput` where the `source` is the census dataset in Amazon S3, and the `destination` is where the script reads this data from, in this case `/opt/ml/processing/input`. These local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to. For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. You also give the ProcessingOutputs values for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The `arguments` parameter in the `run()` method are command-line arguments in our `preprocessing.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_folder,\n",
    "                        s3_data_distribution_type='ShardedByS3Key',\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      arguments=[                          \n",
    "                          's3_output_bucket', bucket\n",
    "                      ]\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 's3://{}/sagemaker/dist-processing/census/output/'.format(bucket)\n",
    "output_folder\n",
    "! aws s3 ls $output_folder --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect the output of the pre-processing job for `001` prefix, which consists of the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.read_csv(output_folder + '001/train_features.csv'.format(bucket), nrows=10)\n",
    "print('Training features shape: {}'.format(training_features.shape))\n",
    "training_features.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
